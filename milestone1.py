# -*- coding: utf-8 -*-
"""milestone1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WselAFvMFNo0-ZdLFfiHIT0RZ4DrL2Qk
"""

import ast
import tokenize
import io
from sentence_transformers import SentenceTransformer

# 10 example snippets covering varied patterns
snippets = [
    "def add(a, b):\n    return a + b",
    "class Greeter:\n    def greet(self, name):\n        print(f\"Hello, {name}!\")",
    "import math",
    "squares = [x * x for x in range(5)]",
    "d = {'one': 1, 'two': 2}",
    "unique = {x for x in [1, 2, 2, 3]}",
    "with open('file.txt') as f:\n    data = f.read()",
    "result = \"yes\" if True else \"no\"",
    "value = obj.attr",
    "match color:\n    case \"red\":\n        print(\"Color is red\")\n    case _:\n        print(\"Unknown color\")",
]

class CodeAnalyzer(ast.NodeVisitor):
    def __init__(self):
        self.functions = []
        self.classes = []
        self.imports = []
        self.patterns = {
            "comprehensions": 0,
            "with_statements": 0,
            "if_expressions": 0,
            "attribute_accesses": 0,
            "match_statements": 0,
        }

    def visit_FunctionDef(self, node):
        self.functions.append(node.name)
        self.generic_visit(node)

    def visit_ClassDef(self, node):
        self.classes.append(node.name)
        self.generic_visit(node)

    def visit_Import(self, node):
        for alias in node.names:
            self.imports.append(alias.name)
        self.generic_visit(node)

    def visit_ImportFrom(self, node):
        for alias in node.names:
            self.imports.append(alias.name)
        self.generic_visit(node)

    def visit_ListComp(self, node):
        self.patterns["comprehensions"] += 1
        self.generic_visit(node)

    def visit_SetComp(self, node):
        self.patterns["comprehensions"] += 1
        self.generic_visit(node)

    def visit_DictComp(self, node):
        self.patterns["comprehensions"] += 1
        self.generic_visit(node)

    def visit_With(self, node):
        self.patterns["with_statements"] += 1
        self.generic_visit(node)

    def visit_IfExp(self, node):
        self.patterns["if_expressions"] += 1
        self.generic_visit(node)

    def visit_Attribute(self, node):
        self.patterns["attribute_accesses"] += 1
        self.generic_visit(node)

    def visit_Match(self, node):
        self.patterns["match_statements"] += 1
        self.generic_visit(node)

def tokenize_code(code):
    tokens = []
    readline = io.BytesIO(code.encode('utf-8')).readline
    for toknum, tokval, _, _, _ in tokenize.tokenize(readline):
        tokens.append((toknum, tokval))
    return tokens

# Analyze snippets and print details
for i, code in enumerate(snippets, 1):
    print(f"Snippet {i}:")
    print(code)

    tree = ast.parse(code)
    analyzer = CodeAnalyzer()
    analyzer.visit(tree)
    tokens = tokenize_code(code)

    print("Extracted Functions:", analyzer.functions)
    print("Extracted Classes:", analyzer.classes)
    print("Extracted Imports:", analyzer.imports)
    print("Code Patterns:", analyzer.patterns)
    print("Tokens (first 10 shown):", tokens[:10])
    print("-" * 50)

# Now encode snippets using MPNet
model_mpnet = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')

# Encoding snippets for semantic embeddings
embeddings_mpnet = model_mpnet.encode(snippets, normalize_embeddings=True)

print(f"Encoded {len(snippets)} snippets into embeddings of shape {embeddings_mpnet.shape}")
print("First snippet embedding sample:", embeddings_mpnet[0][:5])

from transformers import AutoTokenizer

# Initialize DistilRoBERTa tokenizer
tokenizer = AutoTokenizer.from_pretrained("distilroberta-base")

# Example snippets (code as strings)
snippets = [
    "def add(a, b):\n    return a + b",
    "class Greeter:\n    def greet(self, name):\n        print(f\"Hello, {name}!\")",
    "import math",
    "squares = [x * x for x in range(5)]",
    "d = {'one': 1, 'two': 2}",
    "unique = {x for x in [1, 2, 2, 3]}",
    "with open('file.txt') as f:\n    data = f.read()",
    "result = \"yes\" if True else \"no\"",
    "value = obj.attr",
    "match color:\n    case \"red\":\n        print(\"Color is red\")\n    case _:\n        print(\"Unknown color\")",
]

# Tokenize and encode with padding to the longest sequence
encoded_inputs = tokenizer(
    snippets,
    padding=True,         # Pad to the longest snippet
    truncation=True,      # Truncate if too long
    return_tensors="pt"   # Return PyTorch tensors
)

# Print the keys and shapes to confirm
print("Keys in encoded inputs:", encoded_inputs.keys())
print("Input IDs shape:", encoded_inputs['input_ids'].shape)
print("Attention mask shape:", encoded_inputs['attention_mask'].shape)

# Example: print token ids of first snippet (truncated/padded)
print("Token IDs snippet 1:", encoded_inputs['input_ids'][0])

from sentence_transformers import SentenceTransformer

# Load pretrained embedding models
model_minilm = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
model_distilroberta = SentenceTransformer('sentence-transformers/msmarco-distilroberta-base-v2')
model_mpnet = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')

# The code snippets (as text, not tokens) - example subset
snippets = [
    "def add(a, b):\n    return a + b",
    "class Greeter:\n    def greet(self, name):\n        print(f\"Hello, {name}!\")",
    "import math",
    "squares = [x * x for x in range(5)]",
    "unique = {x for x in [1, 2, 2, 3]}",
    "with open('file.txt') as f:\n    data = f.read()",
    "match color:\n    case \"red\":\n        print(\"Color is red\")\n    case _:\n        print(\"Unknown color\")",
]

# Encode each snippet with all models
embeddings_minilm = model_minilm.encode(snippets, normalize_embeddings=True)
embeddings_distilroberta = model_distilroberta.encode(snippets, normalize_embeddings=True)
embeddings_mpnet = model_mpnet.encode(snippets, normalize_embeddings=True)

# Output dimensions and sample embeddings for first snippet for verification
print("MiniLM embedding shape:", embeddings_minilm.shape)
print("MiniLM embedding for snippet 1:", embeddings_minilm[0][:5])  # first 5 values only

print("DistilRoBERTa embedding shape:", embeddings_distilroberta.shape)
print("DistilRoBERTa embedding for snippet 1:", embeddings_distilroberta[0][:5])

print("MPNet embedding shape:", embeddings_mpnet.shape)
print("MPNet embedding for snippet 1:", embeddings_mpnet[0][:5])

