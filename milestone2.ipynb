{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGupDF-kWYUV",
        "outputId": "3f1e811d-8b72-490c-ad0f-64c40a69f4d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your Hugging Face token: HF_TOKEN\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# 1. Setup - install required packages, imports, token input\n",
        "!pip install transformers torch radon matplotlib seaborn ipywidgets --quiet\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import radon.complexity as radon_cc\n",
        "import radon.metrics as radon_metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import pandas as pd\n",
        "\n",
        "# Hugging Face token input (store securely in Colab Secrets if possible)\n",
        "HFTOKEN = input(\"Enter your Hugging Face token: \")\n",
        "\n",
        "# Device configuration\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Utility functions for metrics\n",
        "def evaluate_code_metrics(code):\n",
        "    try:\n",
        "        complexity = radon_cc.cc_visit(code)\n",
        "        cyclomatic = sum(block.complexity for block in complexity)\n",
        "    except Exception:\n",
        "        cyclomatic = None\n",
        "    try:\n",
        "        maintainability = radon_metrics.mi_visit(code, True)\n",
        "    except Exception:\n",
        "        maintainability = None\n",
        "    loc = len(code.split('\\n'))\n",
        "    return {'Cyclomatic Complexity': cyclomatic, 'Maintainability Index': maintainability, 'LOC': loc}\n",
        "\n",
        "# Function to generate python code given model & tokenizer & prompt\n",
        "def generate_code(model, tokenizer, prompt, max_length=256):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_length=max_length)\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded\n",
        "\n",
        "# Prepare a dataframe to collect metrics\n",
        "metrics_df = pd.DataFrame(columns=['Model', 'Prompt', 'Code', 'Cyclomatic Complexity', 'Maintainability Index', 'LOC'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# üîπ Define your Hugging Face token and device\n",
        "HFTOKEN = \"<your_huggingface_token_here>\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# üîπ Global dataframe for collecting metrics\n",
        "metrics_df = pd.DataFrame(columns=['Model', 'Prompt', 'Code', 'Cyclomatic Complexity', 'Maintainability Index', 'LOC'])\n",
        "\n",
        "def evaluate_code_metrics(code):\n",
        "    \"\"\"Compute code quality metrics using Radon.\"\"\"\n",
        "    try:\n",
        "        complexity = radon_cc.cc_visit(code)\n",
        "        cyclomatic = sum(block.complexity for block in complexity)\n",
        "    except Exception:\n",
        "        cyclomatic = None\n",
        "\n",
        "    try:\n",
        "        maintainability = radon_metrics.mi_visit(code, True)\n",
        "    except Exception:\n",
        "        maintainability = None\n",
        "\n",
        "    loc = len(code.split('\\n'))\n",
        "    return {\n",
        "        'Cyclomatic Complexity': cyclomatic,\n",
        "        'Maintainability Index': maintainability,\n",
        "        'LOC': loc\n",
        "    }\n",
        "\n",
        "def generate_code(model, tokenizer, prompt, max_length=256):\n",
        "    \"\"\"Generate code from the model given a text prompt.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_length, do_sample=True, temperature=0.7)\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Remove prompt if repeated\n",
        "    return decoded.replace(prompt, \"\").strip()\n",
        "\n",
        "def model_cell(model_name, hf_model_id):\n",
        "    \"\"\"Create an interactive widget cell for code generation and evaluation.\"\"\"\n",
        "    print(f\"Loading {model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(hf_model_id, token=HFTOKEN)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        hf_model_id,\n",
        "        token=HFTOKEN,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16\n",
        "    )\n",
        "    print(f\"{model_name} loaded and ready!\")\n",
        "\n",
        "    # --- Widgets ---\n",
        "    prompt_input = widgets.Textarea(\n",
        "        value=\"# Write a Python function to compute factorial of a number.\",\n",
        "        placeholder=\"Enter your prompt here\",\n",
        "        description=\"Prompt:\",\n",
        "        layout=widgets.Layout(width=\"100%\", height=\"80px\")\n",
        "    )\n",
        "\n",
        "    generate_button = widgets.Button(description=f\"Generate with {model_name}\")\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    def on_generate(b):\n",
        "        with output_area:\n",
        "            clear_output(wait=True)\n",
        "            prompt = prompt_input.value\n",
        "            print(f\"Prompt:\\n{prompt}\\n\")\n",
        "\n",
        "            generated_code = generate_code(model, tokenizer, prompt)\n",
        "            print(f\"Generated Code:\\n{generated_code}\\n\")\n",
        "\n",
        "            metrics = evaluate_code_metrics(generated_code)\n",
        "            print(f\"Metrics:\")\n",
        "            print(f\"  Cyclomatic Complexity: {metrics['Cyclomatic Complexity']}\")\n",
        "            print(f\"  Maintainability Index: {metrics['Maintainability Index']}\")\n",
        "            print(f\"  Lines of Code (LOC): {metrics['LOC']}\")\n",
        "\n",
        "            # Update global dataframe safely\n",
        "            global metrics_df\n",
        "            new_row = pd.DataFrame([{\n",
        "                'Model': model_name,\n",
        "                'Prompt': prompt,\n",
        "                'Code': generated_code,\n",
        "                **metrics\n",
        "            }])\n",
        "            metrics_df = pd.concat([metrics_df, new_row], ignore_index=True)\n",
        "\n",
        "    generate_button.on_click(on_generate)\n",
        "    display(prompt_input, generate_button, output_area)\n"
      ],
      "metadata": {
        "id": "IZoCQBgTWdDO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Batch testing script (can be in another cell)\n",
        "def batch_test(models_dict, prompts_list):\n",
        "    global metrics_df\n",
        "    for model_name, hf_model in models_dict.items():\n",
        "        print(f\"Batch testing {model_name}...\",flush=True)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(hf_model, use_auth_token=HFTOKEN)\n",
        "        model = AutoModelForCausalLM.from_pretrained(hf_model, use_auth_token=HFTOKEN, device_map=\"auto\", torch_dtype=torch.bfloat16).to(device)\n",
        "        for prompt in prompts_list:\n",
        "            generated_code = generate_code(model, tokenizer, prompt)\n",
        "            metrics = evaluate_code_metrics(generated_code)\n",
        "            new_row = pd.DataFrame([{\n",
        "                'Model': model_name,\n",
        "                'Prompt': prompt,\n",
        "                'Code': generated_code,\n",
        "                'Cyclomatic Complexity': metrics['Cyclomatic Complexity'],\n",
        "                'Maintainability Index': metrics['Maintainability Index'],\n",
        "                'LOC': metrics['LOC']\n",
        "            }])\n",
        "            metrics_df = pd.concat([metrics_df, new_row], ignore_index=True)\n",
        "        print(f\"Completed batch testing for {model_name}.\",flush=True)"
      ],
      "metadata": {
        "id": "ow_Hj9KpYmAy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Visualization cell (use after batch testing)\n",
        "def visualize_metrics():\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    df = metrics_df.dropna()\n",
        "    plt.figure(figsize=(18, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    sns.barplot(data=df, x=\"Model\", y=\"Cyclomatic Complexity\")\n",
        "    plt.title(\"Cyclomatic Complexity by Model\")\n",
        "    plt.subplot(1, 3, 2)\n",
        "    sns.barplot(data=df, x=\"Model\", y=\"Maintainability Index\")\n",
        "    plt.title(\"Maintainability Index by Model\")\n",
        "    plt.subplot(1, 3, 3)\n",
        "    sns.barplot(data=df, x=\"Model\", y=\"LOC\")\n",
        "    plt.title(\"Lines of Code by Model\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example predefined prompts to test\n",
        "sample_prompts = [\n",
        "    \"Write a Python function to compute factorial of a number.\",\n",
        "    \"Implement bubble sort algorithm in Python.\",\n",
        "    \"Create a Python program to read and write files.\",\n",
        "    \"Generate Python code for a simple REST API using Flask.\",\n",
        "    \"Write Python code to connect to a MySQL database.\",\n",
        "    \"Create a Python class for a linked list.\",\n",
        "    \"Write Python code to parse JSON files.\",\n",
        "    \"Write a Python script to scrape data from a website.\",\n",
        "    \"Generate Python code to visualize data using matplotlib.\",\n",
        "    \"Write a Python function to compute Fibonacci numbers.\"\n",
        "]\n",
        "\n",
        "# Dictionary of all models\n",
        "models = {\n",
        "    \"DeepSeek-Coder-1.3B\": \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
        "    \"Phi-2-2.7B\": \"microsoft/phi-2\",\n",
        "    \"Gemma-2B-IT\": \"google/gemma-2b-it\",\n",
        "    \"Stable-Code-3B\": \"stabilityai/stable-code-3b\",\n",
        "    \"Replit-Code-3B\": \"replit/replit-code-v1-3b\"\n",
        "}\n",
        "\n",
        "# To run batch testing in a cell:\n",
        "# batch_test(models, sample_prompts)\n",
        "\n",
        "# To visualize after testing:\n",
        "# visualize_metrics()\n",
        "\n",
        "# You can run each model cell independently like:\n",
        "# model_cell(\"DeepSeek-Coder-1.3B\", \"deepseek-ai/deepseek-coder-1.3b-instruct\")"
      ],
      "metadata": {
        "id": "3R9aodgGY3FM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EryKCLyVZC6b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}